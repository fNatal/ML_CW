{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261bfec4",
   "metadata": {},
   "source": [
    "# Machine Learning CourseworK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715583ad",
   "metadata": {},
   "source": [
    "Natalie Fernando: 20222466/2312542"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bb73d76",
   "metadata": {},
   "source": [
    "PREPROCESSING STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c3da2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_dataset/bank-additional-full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19296\\2837693716.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'path_to_dataset/bank-additional-full.csv'\u001b[0m  \u001b[1;31m# Update with your dataset path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Step 1: Exploratory Data Analysis (EDA)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_dataset/bank-additional-full.csv'"
     ]
    }
   ],
   "source": [
    "#pip install ucimlrepo , download if the libaries dosenot exist \n",
    "#pip install imblearn\n",
    "\n",
    " #Import required libraries\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 1: Fetch the dataset using the ucimlrepo library\n",
    "bank_marketing = fetch_ucirepo(id=222)  # Bank Marketing Dataset\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = bank_marketing.data.features\n",
    "y = bank_marketing.data.targets\n",
    "\n",
    "# Display dataset metadata and structure\n",
    "print(\"Dataset Metadata:\\n\", bank_marketing.metadata)\n",
    "print(\"\\nDataset Variables:\\n\", bank_marketing.variables)\n",
    "print(\"\\nFeatures Sample:\\n\", X.head())\n",
    "print(\"\\nTarget Sample:\\n\", y.head())\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "print(\"\\nMissing Values Check:\\n\", X.isnull().sum())\n",
    "# No missing values are expected, but we can still fill any that arise\n",
    "X.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Step 3: Drop Irrelevant Features\n",
    "# 'duration' is a leakage feature and must be removed if present\n",
    "X = X.drop(columns=['duration'], errors='ignore')\n",
    "\n",
    "# Step 4: Create New Features\n",
    "# Bin 'age' into categories\n",
    "X['age_group'] = pd.cut(X['age'], bins=[0, 25, 45, 65, 100], labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "# One-hot encode the new 'age_group' feature\n",
    "X = pd.get_dummies(X, columns=['age_group'], drop_first=True)\n",
    "\n",
    "# Step 5: Encode Categorical Variables\n",
    "# One-hot encode all nominal categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Encode the target variable (y) using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 6: Handle Class Imbalance\n",
    "# Apply SMOTE to balance the target classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y_encoded)\n",
    "print(\"\\nBalanced Class Distribution:\\n\", pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# Step 7: Rescale Data\n",
    "# Standardize numerical features for Neural Network compatibility\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_balanced)\n",
    "\n",
    "# Step 8: Split the Dataset\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Final shapes\n",
    "print(\"\\nFinal Training and Testing Data Shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9268700",
   "metadata": {},
   "source": [
    "RANDOM FROEST USING 50 TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, sample_size=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = np.random.choice(len(y), self.sample_size or len(y), replace=True)\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X[idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Collect predictions from all trees\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # Display each tree's prediction\n",
    "        print(\"\\nPredictions from each tree:\")\n",
    "        for i, tree_pred in enumerate(tree_preds, start=1):\n",
    "            print(f\"Tree {i}: {tree_pred}\")\n",
    "        \n",
    "        # Majority voting for each sample\n",
    "        majority_predictions = np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=tree_preds)\n",
    "        \n",
    "        # Final majority decision for all samples\n",
    "        overall_majority = Counter(majority_predictions).most_common(1)[0][0]\n",
    "        \n",
    "        # Convert to \"Yes\" or \"No\"\n",
    "        final_decision = \"Yes\" if overall_majority == 1 else \"No\"\n",
    "        \n",
    "        # Print the final outcome message\n",
    "        print(f\"\\nFinal Prediction Based on Majority Voting: {final_decision}\")\n",
    "        return majority_predictions\n",
    "\n",
    "\n",
    "rf = RandomForest(n_estimators=50, max_depth=5)  # Using 50 trees\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4972c",
   "metadata": {},
   "source": [
    "Neural Network with 10 Hidden layers with each hidden layer containing 10 neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_layer = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output_layer\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        error = y - output\n",
    "        output_delta = error * self.sigmoid_derivative(output)\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_layer.T, output_delta) * self.learning_rate\n",
    "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        predictions = (output > 0.5).astype(int)\n",
    "        \n",
    "        # Display outputs for each sample\n",
    "        print(\"\\nOutputs for each sample from Neural Network:\")\n",
    "        for i, pred in enumerate(output.flatten(), start=1):\n",
    "            print(f\"Sample {i}: {'Yes' if pred > 0.5 else 'No'} (Raw Output: {pred:.4f})\")\n",
    "        \n",
    "        # Majority decision for all samples\n",
    "        overall_majority = Counter(predictions.flatten()).most_common(1)[0][0]\n",
    "        final_decision = \"Yes\" if overall_majority == 1 else \"No\"\n",
    "        \n",
    "        # Print the final outcome message\n",
    "        print(f\"\\nFinal Prediction Based on Majority Decision: {final_decision}\")\n",
    "        return predictions\n",
    "\n",
    "# Example Usage\n",
    "# Assuming X_train and y_train are preprocessed NumPy arrays\n",
    "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=10, output_size=1)  # 10 hidden layers\n",
    "nn.train(X_train, y_train, epochs=1000)\n",
    "y_pred_nn = nn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce0e15",
   "metadata": {},
   "source": [
    "# Model evalulation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36306d70",
   "metadata": {},
   "source": [
    "Model evalulation, accuracy, Classification Report  and confusion matrix for Random Forest during traing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "classification_reports = {}\n",
    "accuracies = {}\n",
    "\n",
    "# Evaluate each Random Forest model (trees trained in the custom implementation)\n",
    "for n_trees, rf in rf_models.items():  # rf_models: dictionary containing trained RF models\n",
    "    print(f\"\\nEvaluating Random Forest with {n_trees} trees...\")\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_rf = rf.predict(X_test_manual)  # Replace X_test_manual and y_test_manual with actual variables\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_manual, y_pred_rf)\n",
    "    accuracies[n_trees] = accuracy\n",
    "    print(f\"Accuracy for {n_trees} trees: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generate and store classification report\n",
    "    report = classification_report(y_test_manual, y_pred_rf, target_names=[\"No\", \"Yes\"])\n",
    "    classification_reports[n_trees] = report\n",
    "    print(f\"\\nClassification Report for {n_trees} trees:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_rf, title=f\"Confusion Matrix for {n_trees} trees\")\n",
    "\n",
    "# Display overall accuracies\n",
    "print(\"\\nRandom Forest Accuracies by Number of Trees:\")\n",
    "for n_trees, acc in accuracies.items():\n",
    "    print(f\"{n_trees} Trees: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efdb9d02",
   "metadata": {},
   "source": [
    "Model evaluation, accuracy, Classification Report  and confusion matrix for Random forest for testing the model using the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix (Test Data)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store test results\n",
    "test_rf_results = {}\n",
    "test_classification_reports = {}\n",
    "\n",
    "# Evaluate each Random Forest model on the test data\n",
    "for n_trees, rf in rf_models.items():  # rf_models: dictionary containing trained RF models\n",
    "    print(f\"\\nEvaluating Random Forest with {n_trees} trees on Test Data...\")\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred_test = rf.predict(X_test_manual)  # Replace `X_test_manual` and `y_test_manual` with actual variables\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_manual, y_pred_test)\n",
    "    test_rf_results[n_trees] = accuracy\n",
    "    \n",
    "    # Generate and store classification report\n",
    "    report = classification_report(y_test_manual, y_pred_test, target_names=[\"No\", \"Yes\"])\n",
    "    test_classification_reports[n_trees] = report\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Results for {n_trees} trees:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_test, title=f\"Confusion Matrix for {n_trees} trees (Test Data)\")\n",
    "\n",
    "# Display overall test accuracies\n",
    "print(\"\\nOverall Test Accuracies for Random Forest:\")\n",
    "for n_trees, acc in test_rf_results.items():\n",
    "    print(f\"{n_trees} Trees: {acc:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86bc9d6b",
   "metadata": {},
   "source": [
    "Tetsing the model with new data for the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate new test data (10 new instances)\n",
    "# Assuming the feature structure matches the training data\n",
    "new_data = {\n",
    "    'age': np.random.randint(18, 70, 10),\n",
    "    'balance': np.random.randint(-1000, 5000, 10),\n",
    "    'day': np.random.randint(1, 31, 10),\n",
    "    'campaign': np.random.randint(1, 10, 10),\n",
    "    'pdays': np.random.randint(-1, 300, 10),\n",
    "    'previous': np.random.randint(0, 10, 10),\n",
    "    'job_blue-collar': np.random.randint(0, 2, 10),\n",
    "    'job_entrepreneur': np.random.randint(0, 2, 10),\n",
    "    'job_housemaid': np.random.randint(0, 2, 10),\n",
    "    'job_management': np.random.randint(0, 2, 10),\n",
    "    'job_retired': np.random.randint(0, 2, 10),\n",
    "    'job_self-employed': np.random.randint(0, 2, 10),\n",
    "    'job_services': np.random.randint(0, 2, 10),\n",
    "    'job_student': np.random.randint(0, 2, 10),\n",
    "    'job_technician': np.random.randint(0, 2, 10),\n",
    "    'job_unemployed': np.random.randint(0, 2, 10),\n",
    "    'job_unknown': np.random.randint(0, 2, 10),\n",
    "    'marital_married': np.random.randint(0, 2, 10),\n",
    "    'marital_single': np.random.randint(0, 2, 10),\n",
    "    'education_secondary': np.random.randint(0, 2, 10),\n",
    "    'education_tertiary': np.random.randint(0, 2, 10),\n",
    "    'education_unknown': np.random.randint(0, 2, 10),\n",
    "    'default_yes': np.random.randint(0, 2, 10),\n",
    "    'housing_yes': np.random.randint(0, 2, 10),\n",
    "    'loan_yes': np.random.randint(0, 2, 10),\n",
    "    'contact_telephone': np.random.randint(0, 2, 10),\n",
    "    'contact_unknown': np.random.randint(0, 2, 10),\n",
    "    'month_aug': np.random.randint(0, 2, 10),\n",
    "    'month_dec': np.random.randint(0, 2, 10),\n",
    "    'month_feb': np.random.randint(0, 2, 10),\n",
    "    'month_jan': np.random.randint(0, 2, 10),\n",
    "    'month_jul': np.random.randint(0, 2, 10),\n",
    "    'month_jun': np.random.randint(0, 2, 10),\n",
    "    'month_mar': np.random.randint(0, 2, 10),\n",
    "    'month_may': np.random.randint(0, 2, 10),\n",
    "    'month_nov': np.random.randint(0, 2, 10),\n",
    "    'month_oct': np.random.randint(0, 2, 10),\n",
    "    'month_sep': np.random.randint(0, 2, 10),\n",
    "    'poutcome_other': np.random.randint(0, 2, 10),\n",
    "    'poutcome_success': np.random.randint(0, 2, 10),\n",
    "    'poutcome_unknown': np.random.randint(0, 2, 10),\n",
    "}\n",
    "\n",
    "# Convert the new data into a DataFrame\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Rescale the new data using the previously fitted scaler\n",
    "new_data_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "# Store predictions for all Random Forest configurations\n",
    "new_predictions = {}\n",
    "\n",
    "for n_trees, rf in rf_models.items():  # rf_models contains trained RF models\n",
    "    # Predict the outcomes for the new data\n",
    "    y_pred_new = rf.predict(new_data_scaled)\n",
    "    new_predictions[n_trees] = y_pred_new\n",
    "    print(f\"\\nPredictions for {n_trees} trees:\")\n",
    "    print(y_pred_new)\n",
    "\n",
    "# Display predictions for all configurations\n",
    "print(\"\\nSummary of Predictions for New Data:\")\n",
    "for n_trees, predictions in new_predictions.items():\n",
    "    print(f\"{n_trees} Trees: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23c1271e",
   "metadata": {},
   "source": [
    "Evaluation During Validation - Random Forest\n",
    "Typically evaluate models using a separate validation set (which is distinct from the training and test sets) to ensure that the model generalizes well to unseen data during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ad4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix (Validation Set)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store validation results\n",
    "validation_rf_results = {}\n",
    "validation_classification_reports = {}\n",
    "\n",
    "# Evaluate each Random Forest model on the test data (acting as the validation set)\n",
    "for n_trees, rf in rf_models.items():  # rf_models contains trained RF models\n",
    "    print(f\"\\nEvaluating Random Forest with {n_trees} trees on Validation Set...\")\n",
    "    \n",
    "    # Make predictions on the test data (validation set)\n",
    "    y_pred_validation = rf.predict(X_test_manual)  # Replace X_test_manual and y_test_manual with actual variables\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_validation = accuracy_score(y_test_manual, y_pred_validation)\n",
    "    validation_rf_results[n_trees] = accuracy_validation\n",
    "    \n",
    "    # Generate and store classification report\n",
    "    report = classification_report(y_test_manual, y_pred_validation, target_names=[\"No\", \"Yes\"])\n",
    "    validation_classification_reports[n_trees] = report\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Validation Results for {n_trees} trees:\")\n",
    "    print(f\"Accuracy: {accuracy_validation:.2f}\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_validation, title=f\"Confusion Matrix for {n_trees} trees (Validation)\")\n",
    "\n",
    "# Display overall validation accuracies\n",
    "print(\"\\nOverall Validation Accuracies for Random Forest:\")\n",
    "for n_trees, acc in validation_rf_results.items():\n",
    "    print(f\"{n_trees} Trees: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86e05067",
   "metadata": {},
   "source": [
    "Model evalulation, accuracy, Classification Report  and confusion matrix for Neural NetworK (MLP) during traing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd500259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# List to store results\n",
    "nn_results = []\n",
    "\n",
    "# Train and evaluate the custom Neural Network with varying hidden layers\n",
    "for hidden_layers in [2, 4, 8, 10, 16, 20, 25]:\n",
    "    print(f\"\\nTraining Neural Network with {hidden_layers} hidden layers...\")\n",
    "    \n",
    "    # Create and train the Neural Network model\n",
    "    nn = NeuralNetwork(input_size=X_train_manual.shape[1], hidden_size=hidden_layers, output_size=1)\n",
    "    nn.train(X_train_manual, y_train_manual, epochs=1000)\n",
    "    \n",
    "    # Predict using the trained Neural Network\n",
    "    y_pred_nn = nn.predict(X_test_manual).flatten()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_nn = accuracy_score(y_test_manual, y_pred_nn)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report_nn = classification_report(y_test_manual, y_pred_nn, target_names=[\"No\", \"Yes\"])\n",
    "    \n",
    "    # Store results\n",
    "    nn_results.append((hidden_layers, accuracy_nn, report_nn))\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Results for {hidden_layers} hidden layers:\")\n",
    "    print(f\"Accuracy: {accuracy_nn:.2f}\")\n",
    "    print(report_nn)\n",
    "    \n",
    "    # Confusion Matrix for each model\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_nn, title=f\"Confusion Matrix for {hidden_layers} hidden layers (NN)\")\n",
    "\n",
    "# Display the results summary for Neural Network\n",
    "nn_results_summary = {result[0]: result[1] for result in nn_results}\n",
    "print(\"\\nSummary of Results for Neural Network:\")\n",
    "for hidden_layers, accuracy in nn_results_summary.items():\n",
    "    print(f\"{hidden_layers} Hidden Layers: Accuracy = {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "521ea667",
   "metadata": {},
   "source": [
    "Model evaluation, accuracy, Classification Report  and confusion matrix for NN for testing the model using the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix (Test Data)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store test results\n",
    "test_nn_results = {}\n",
    "test_nn_classification_reports = {}\n",
    "\n",
    "# Evaluate custom Neural Network with different hidden layer configurations on the test data\n",
    "for hidden_layers in [2, 4, 8, 10, 16, 20, 25]:\n",
    "    print(f\"\\nEvaluating Neural Network with {hidden_layers} hidden layers on test data...\")\n",
    "    \n",
    "    # Create and train Neural Network model with specified number of hidden layers\n",
    "    nn = NeuralNetwork(input_size=X_train_manual.shape[1], hidden_size=hidden_layers, output_size=1)\n",
    "    nn.train(X_train_manual, y_train_manual, epochs=1000)\n",
    "    \n",
    "    # Predict using the trained Neural Network\n",
    "    y_pred_nn_test = nn.predict(X_test_manual).flatten()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_test_nn = accuracy_score(y_test_manual, y_pred_nn_test)\n",
    "    test_nn_results[hidden_layers] = accuracy_test_nn\n",
    "    \n",
    "    # Generate classification report\n",
    "    report_nn_test = classification_report(y_test_manual, y_pred_nn_test, target_names=[\"No\", \"Yes\"])\n",
    "    test_nn_classification_reports[hidden_layers] = report_nn_test\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Results for {hidden_layers} hidden layers:\")\n",
    "    print(f\"Accuracy: {accuracy_test_nn:.2f}\")\n",
    "    print(report_nn_test)\n",
    "    \n",
    "    # Confusion Matrix for test data\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_nn_test, title=f\"Confusion Matrix for {hidden_layers} hidden layers (NN - Test)\")\n",
    "\n",
    "# Display overall test results\n",
    "print(\"\\nOverall Test Accuracies for Neural Network:\")\n",
    "for hidden_layers, acc in test_nn_results.items():\n",
    "    print(f\"{hidden_layers} Hidden Layers: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8a6c2",
   "metadata": {},
   "source": [
    "Tetsing the model with new data for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate new data (10 new instances)\n",
    "new_data = {\n",
    "    'age': np.random.randint(18, 70, 10),\n",
    "    'balance': np.random.randint(-1000, 5000, 10),\n",
    "    'day': np.random.randint(1, 31, 10),\n",
    "    'campaign': np.random.randint(1, 10, 10),\n",
    "    'pdays': np.random.randint(-1, 300, 10),\n",
    "    'previous': np.random.randint(0, 10, 10),\n",
    "    'job_blue-collar': np.random.randint(0, 2, 10),\n",
    "    'job_entrepreneur': np.random.randint(0, 2, 10),\n",
    "    'job_housemaid': np.random.randint(0, 2, 10),\n",
    "    'job_management': np.random.randint(0, 2, 10),\n",
    "    'job_retired': np.random.randint(0, 2, 10),\n",
    "    'job_self-employed': np.random.randint(0, 2, 10),\n",
    "    'job_services': np.random.randint(0, 2, 10),\n",
    "    'job_student': np.random.randint(0, 2, 10),\n",
    "    'job_technician': np.random.randint(0, 2, 10),\n",
    "    'job_unemployed': np.random.randint(0, 2, 10),\n",
    "    'job_unknown': np.random.randint(0, 2, 10),\n",
    "    'marital_married': np.random.randint(0, 2, 10),\n",
    "    'marital_single': np.random.randint(0, 2, 10),\n",
    "    'education_secondary': np.random.randint(0, 2, 10),\n",
    "    'education_tertiary': np.random.randint(0, 2, 10),\n",
    "    'education_unknown': np.random.randint(0, 2, 10),\n",
    "    'default_yes': np.random.randint(0, 2, 10),\n",
    "    'housing_yes': np.random.randint(0, 2, 10),\n",
    "    'loan_yes': np.random.randint(0, 2, 10),\n",
    "    'contact_telephone': np.random.randint(0, 2, 10),\n",
    "    'contact_unknown': np.random.randint(0, 2, 10),\n",
    "    'month_aug': np.random.randint(0, 2, 10),\n",
    "    'month_dec': np.random.randint(0, 2, 10),\n",
    "    'month_feb': np.random.randint(0, 2, 10),\n",
    "    'month_jan': np.random.randint(0, 2, 10),\n",
    "    'month_jul': np.random.randint(0, 2, 10),\n",
    "    'month_jun': np.random.randint(0, 2, 10),\n",
    "    'month_mar': np.random.randint(0, 2, 10),\n",
    "    'month_may': np.random.randint(0, 2, 10),\n",
    "    'month_nov': np.random.randint(0, 2, 10),\n",
    "    'month_oct': np.random.randint(0, 2, 10),\n",
    "    'month_sep': np.random.randint(0, 2, 10),\n",
    "    'poutcome_other': np.random.randint(0, 2, 10),\n",
    "    'poutcome_success': np.random.randint(0, 2, 10),\n",
    "    'poutcome_unknown': np.random.randint(0, 2, 10),\n",
    "}\n",
    "\n",
    "# Convert the new data into a DataFrame\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Ensure that the new data has the same feature structure as the training data\n",
    "missing_columns = [col for col in X.columns if col not in new_data_df.columns]\n",
    "for col in missing_columns:\n",
    "    new_data_df[col] = 0  # Assigning 0 to the missing columns\n",
    "\n",
    "# Reorder the columns to match the original training data\n",
    "new_data_df = new_data_df[X.columns]\n",
    "\n",
    "# Rescale the new data using the previously fitted scaler\n",
    "new_data_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "# Store predictions for all hidden layer configurations\n",
    "new_predictions_nn = {}\n",
    "\n",
    "for hidden_layers in [2, 4, 8, 10, 16, 20, 25]:\n",
    "    print(f\"\\nGenerating predictions for Neural Network with {hidden_layers} hidden layers...\")\n",
    "    \n",
    "    # Create and train Neural Network model with specified number of hidden layers\n",
    "    nn = NeuralNetwork(input_size=X_train_manual.shape[1], hidden_size=hidden_layers, output_size=1)\n",
    "    nn.train(X_train_manual, y_train_manual, epochs=1000)\n",
    "    \n",
    "    # Predict the outcomes for the new data\n",
    "    y_pred_new_nn = nn.predict(new_data_scaled).flatten()\n",
    "    new_predictions_nn[hidden_layers] = y_pred_new_nn\n",
    "\n",
    "    # Display the predictions for the current configuration\n",
    "    print(f\"Predictions for {hidden_layers} hidden layers:\")\n",
    "    print(y_pred_new_nn)\n",
    "\n",
    "# Display all predictions\n",
    "print(\"\\nSummary of Predictions for New Data:\")\n",
    "for hidden_layers, predictions in new_predictions_nn.items():\n",
    "    print(f\"{hidden_layers} Hidden Layers: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aed377",
   "metadata": {},
   "source": [
    "Evaluation During Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix (Validation Set)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store validation results\n",
    "validation_nn_results = {}\n",
    "validation_nn_classification_reports = {}\n",
    "\n",
    "# Evaluate custom Neural Network with different hidden layer configurations\n",
    "for hidden_layers in [2, 4, 8, 10, 16, 20, 25]:\n",
    "    print(f\"\\nEvaluating Neural Network with {hidden_layers} hidden layers on validation (test) data...\")\n",
    "    \n",
    "    # Create and train Neural Network model with specified number of hidden layers\n",
    "    nn = NeuralNetwork(input_size=X_train_manual.shape[1], hidden_size=hidden_layers, output_size=1)\n",
    "    nn.train(X_train_manual, y_train_manual, epochs=1000)\n",
    "    \n",
    "    # Predict using the trained Neural Network\n",
    "    y_pred_nn_val = nn.predict(X_test_manual).flatten()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_nn_val = accuracy_score(y_test_manual, y_pred_nn_val)\n",
    "    validation_nn_results[hidden_layers] = accuracy_nn_val\n",
    "    \n",
    "    # Generate classification report\n",
    "    report_nn_val = classification_report(y_test_manual, y_pred_nn_val, target_names=[\"No\", \"Yes\"])\n",
    "    validation_nn_classification_reports[hidden_layers] = report_nn_val\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Validation Results for {hidden_layers} hidden layers:\")\n",
    "    print(f\"Accuracy: {accuracy_nn_val:.2f}\")\n",
    "    print(report_nn_val)\n",
    "    \n",
    "    # Confusion Matrix for validation data\n",
    "    plot_confusion_matrix(y_test_manual, y_pred_nn_val, title=f\"Confusion Matrix for {hidden_layers} hidden layers (NN - Validation)\")\n",
    "\n",
    "# Display overall validation results\n",
    "print(\"\\nOverall Validation Accuracies for Neural Network:\")\n",
    "for hidden_layers, acc in validation_nn_results.items():\n",
    "    print(f\"{hidden_layers} Hidden Layers: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186877c6",
   "metadata": {},
   "source": [
    "# A/B Testing for the RF and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054a115",
   "metadata": {},
   "source": [
    "Need to compare how each model performs on the new values. Since we don't have true labels for the new data, we can approach this in the following ways:\n",
    "\n",
    "Consistency: Compare whether both models (Random Forest and MLP) make similar predictions. If they predict similarly, the models might be agreeing on the classification of the new data.\n",
    "Diversity of Predictions: Compare how often the models disagree, which can give us insight into their relative strengths and weaknesses.\n",
    "\n",
    "Group A- RF\n",
    "Group B- NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Predictions for Random Forest (50 trees)\n",
    "rf_predictions_new = [0, 1, 0, 1, 1, 1, 0, 0, 0, 0]  # Replace with actual predictions from RF model\n",
    "\n",
    "# Example Predictions for Neural Network (10 hidden layers)\n",
    "nn_predictions_new = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  # Replace with actual predictions from NN model\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = sum(1 for i in range(len(rf_predictions_new)) if rf_predictions_new[i] == nn_predictions_new[i])\n",
    "disagreement_count = len(rf_predictions_new) - agreement_count\n",
    "\n",
    "# Calculate percentage agreement and disagreement\n",
    "total_comparisons = len(rf_predictions_new)\n",
    "agreement_percentage = (agreement_count / total_comparisons) * 100\n",
    "disagreement_percentage = (disagreement_count / total_comparisons) * 100\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nComparison of Predictions:\")\n",
    "print(f\"Agreement: {agreement_percentage:.2f}%\")\n",
    "print(f\"Disagreement: {disagreement_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
